{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d5b8923",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f690c9a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/aditya/git/columbia_advanced_database_project_2\n"
     ]
    }
   ],
   "source": [
    "%cd /home/aditya/git/columbia_advanced_database_project_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f515aa22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aditya/.pyenv/versions/my_env/lib/python3.6/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import fire\n",
    "import spacy\n",
    "\n",
    "import display\n",
    "import entities\n",
    "import search\n",
    "import validation\n",
    "from spanbert import SpanBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1976a5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = 'AIzaSyBx4fnqPbVC0BPRJxdIC0S3KR2PHNg2qPU'\n",
    "engine_id = 'e8f624bdc54650190'\n",
    "relation = 1\n",
    "threshold = 0.7\n",
    "query = 'mark zuckerberg harvard'\n",
    "k = 17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c1b7e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = dict()\n",
    "seen_sites = set()\n",
    "previous_queries = set()\n",
    "i = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d84fb742",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation.validate_api_key(api_key)\n",
    "validation.validate_engine_id(engine_id)\n",
    "validation.validate_relation(relation)\n",
    "validation.validate_threshold(threshold)\n",
    "validation.validate_query(query)\n",
    "validation.validate_k(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "060d2983",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____\n",
      "Parameters:\n",
      "Client key\t= AIzaSyBx4fnqPbVC0BPRJxdIC0S3KR2PHNg2qPU\n",
      "Engine key\t= e8f624bdc54650190\n",
      "Relation\t= per:schools_attended\n",
      "Threshold\t= 0.7\n",
      "Query\t\t= mark zuckerberg harvard\n",
      "# of Tuples\t= 17\n",
      "Loading necessary libraries; This should take a minute or so ...)\n"
     ]
    }
   ],
   "source": [
    "display.display_parameters(api_key, engine_id, relation, threshold, query, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "737d495d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-trained spanBERT from pretrained_spanbert\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "spanbert = SpanBERT(\"pretrained_spanbert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "95acda3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = (['Mark', 'Zuckerberg', '-', 'Wikipedia', 'Mark', 'Zuckerberg', 'From', 'Wikipedia', ','], ('Mark Zuckerberg - Wikipedia', 'PERSON', (0, 3)), ('Wikipedia', 'ORGANIZATION', (7, 7)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eb2c6833",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Mark',\n",
       " 'Zuckerberg',\n",
       " '-',\n",
       " 'Wikipedia',\n",
       " 'Mark',\n",
       " 'Zuckerberg',\n",
       " 'From',\n",
       " 'Wikipedia',\n",
       " ',']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "62eaa1c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Mark Zuckerberg - Wikipedia', 'PERSON', (0, 3))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "90e97550",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========== Iteration: 1 - Query: mark zuckerberg harvard ===========\n",
      "\n",
      "\n",
      "URL ( 1 / 10): https://en.wikipedia.org/wiki/Mark_Zuckerberg\n",
      "Fetching text from url ...\n",
      "Trimming webpage content from 85736 to 19995 characters\n",
      "Webpage length (num characters): 19995\n",
      "Annotating the webpage using spacy...\n",
      "Extracted 154 sentences. Processing each sentence one by one to check for presence of right pair of named entity types; if so, will run the second pipeline ...\n",
      "Processed 0 / 154 sentences\n",
      "Processed 5 / 154 sentences\n",
      "Processed 10 / 154 sentences\n",
      "Processed 15 / 154 sentences\n",
      "Processed 20 / 154 sentences\n",
      "Processed 25 / 154 sentences\n",
      "Processed 30 / 154 sentences\n",
      "Processed 35 / 154 sentences\n",
      "Processed 40 / 154 sentences\n",
      "Processed 45 / 154 sentences\n",
      "Processed 50 / 154 sentences\n",
      "Processed 55 / 154 sentences\n",
      "Processed 60 / 154 sentences\n",
      "Processed 65 / 154 sentences\n",
      "Processed 70 / 154 sentences\n",
      "Processed 75 / 154 sentences\n",
      "Processed 80 / 154 sentences\n",
      "Processed 85 / 154 sentences\n",
      "Processed 90 / 154 sentences\n",
      "Processed 95 / 154 sentences\n",
      "Processed 100 / 154 sentences\n",
      "Processed 105 / 154 sentences\n",
      "Processed 110 / 154 sentences\n",
      "Processed 115 / 154 sentences\n",
      "Processed 120 / 154 sentences\n",
      "Processed 125 / 154 sentences\n",
      "Processed 130 / 154 sentences\n",
      "Processed 135 / 154 sentences\n",
      "Processed 140 / 154 sentences\n",
      "Processed 145 / 154 sentences\n",
      "Processed 150 / 154 sentences\n",
      "\n",
      "=== Extracted Relation ===\n",
      "Input tokens: ['The', 'New', 'Yorker', 'noted', 'that', 'by', 'the', 'time', 'Zuckerberg', 'began', 'classes', 'at', 'Harvard', 'in', '2002', ',']\n",
      "Output Confidence: 0.7307987213134766 ; Subject: Zuckerberg ; Object: Harvard ;\n",
      "Adding to set of extracted relations\n",
      "==========\n",
      "\n",
      "=== Extracted Relation ===\n",
      "Input tokens: ['Zuckerberg', ',', 'after', 'giving', 'a', 'commencement', 'speech', ',', '[', '62', ']', 'received', 'an', 'honorary', 'degree', 'from', 'Harvard', '.']\n",
      "Output Confidence: 0.9893775582313538 ; Subject: Zuckerberg ; Object: Harvard ;\n",
      "Adding to set of extracted relations\n",
      "==========\n",
      "Extracted annotations for  2  out of total  154  sentences\n",
      "Relations extracted from this website: 1 (Overall: 0)\n",
      "\n",
      "\n",
      "URL ( 2 / 10): https://news.harvard.edu/gazette/story/2017/05/mark-zuckerbergs-speech-as-written-for-harvards-class-of-2017/\n",
      "Fetching text from url ...\n",
      "Trimming webpage content from 22942 to 19998 characters\n",
      "Webpage length (num characters): 19998\n",
      "Annotating the webpage using spacy...\n",
      "Extracted 238 sentences. Processing each sentence one by one to check for presence of right pair of named entity types; if so, will run the second pipeline ...\n",
      "Processed 0 / 238 sentences\n",
      "Processed 5 / 238 sentences\n",
      "Processed 10 / 238 sentences\n",
      "Processed 15 / 238 sentences\n",
      "Processed 20 / 238 sentences\n",
      "Processed 25 / 238 sentences\n",
      "Processed 30 / 238 sentences\n",
      "Processed 35 / 238 sentences\n",
      "Processed 40 / 238 sentences\n",
      "Processed 45 / 238 sentences\n",
      "Processed 50 / 238 sentences\n",
      "Processed 55 / 238 sentences\n",
      "Processed 60 / 238 sentences\n",
      "Processed 65 / 238 sentences\n",
      "Processed 70 / 238 sentences\n",
      "Processed 75 / 238 sentences\n",
      "Processed 80 / 238 sentences\n",
      "Processed 85 / 238 sentences\n",
      "Processed 90 / 238 sentences\n",
      "Processed 95 / 238 sentences\n",
      "Processed 100 / 238 sentences\n",
      "Processed 105 / 238 sentences\n",
      "Processed 110 / 238 sentences\n",
      "Processed 115 / 238 sentences\n",
      "Processed 120 / 238 sentences\n",
      "Processed 125 / 238 sentences\n",
      "Processed 130 / 238 sentences\n",
      "Processed 135 / 238 sentences\n",
      "Processed 140 / 238 sentences\n",
      "Processed 145 / 238 sentences\n",
      "Processed 150 / 238 sentences\n",
      "Processed 155 / 238 sentences\n",
      "Processed 160 / 238 sentences\n",
      "Processed 165 / 238 sentences\n",
      "Processed 170 / 238 sentences\n",
      "Processed 175 / 238 sentences\n",
      "Processed 180 / 238 sentences\n",
      "Processed 185 / 238 sentences\n",
      "Processed 190 / 238 sentences\n",
      "Processed 195 / 238 sentences\n",
      "Processed 200 / 238 sentences\n",
      "Processed 205 / 238 sentences\n",
      "Processed 210 / 238 sentences\n",
      "Processed 215 / 238 sentences\n",
      "Processed 220 / 238 sentences\n",
      "Processed 225 / 238 sentences\n",
      "Processed 230 / 238 sentences\n",
      "Processed 235 / 238 sentences\n",
      "\n",
      "=== Extracted Relation ===\n",
      "Input tokens: ['I', 'met', 'David', 'Razu', 'Aznar', ',', 'graduating', 'from', 'the', 'Kennedy', 'School', 'today', '.']\n",
      "Output Confidence: 0.9921025037765503 ; Subject: David Razu Aznar ; Object: the Kennedy School ;\n",
      "Adding to set of extracted relations\n",
      "==========\n",
      "Extracted annotations for  1  out of total  238  sentences\n",
      "Relations extracted from this website: 1 (Overall: 1)\n",
      "\n",
      "\n",
      "URL ( 3 / 10): https://www.cnbc.com/2017/05/25/mark-zuckerberg-returns-to-the-harvard-dorm-where-facebook-was-born.html\n",
      "Fetching text from url ...\n",
      "Webpage length (num characters): 3815\n",
      "Annotating the webpage using spacy...\n",
      "Extracted 28 sentences. Processing each sentence one by one to check for presence of right pair of named entity types; if so, will run the second pipeline ...\n",
      "Processed 0 / 28 sentences\n",
      "Processed 5 / 28 sentences\n",
      "Processed 10 / 28 sentences\n",
      "Processed 15 / 28 sentences\n",
      "Processed 20 / 28 sentences\n",
      "Processed 25 / 28 sentences\n",
      "Extracted annotations for  0  out of total  28  sentences\n",
      "Relations extracted from this website: 0 (Overall: 2)\n",
      "\n",
      "\n",
      "URL ( 4 / 10): https://www.forbes.com/sites/susanadams/2021/12/08/zuckerberg-and-chan-pledge-500-million-of-their-facebook-fortune-to-create-an-ai-institute-at-harvard/\n",
      "Fetching text from url ...\n",
      "Webpage length (num characters): 8586\n",
      "Annotating the webpage using spacy...\n",
      "Extracted 57 sentences. Processing each sentence one by one to check for presence of right pair of named entity types; if so, will run the second pipeline ...\n",
      "Processed 0 / 57 sentences\n",
      "Processed 5 / 57 sentences\n",
      "Processed 10 / 57 sentences\n",
      "Processed 15 / 57 sentences\n",
      "Processed 20 / 57 sentences\n",
      "Processed 25 / 57 sentences\n",
      "Processed 30 / 57 sentences\n",
      "Processed 35 / 57 sentences\n",
      "Processed 40 / 57 sentences\n",
      "Processed 45 / 57 sentences\n",
      "Processed 50 / 57 sentences\n",
      "Processed 55 / 57 sentences\n",
      "\n",
      "=== Extracted Relation ===\n",
      "Input tokens: ['Zuckerberg', 'and', 'Chan', ',', 'who', 'met', 'at', 'a', 'Harvard', 'fraternity', 'party', 'in', '2003', ',']\n",
      "Output Confidence: 0.7690868973731995 ; Subject: Chan ; Object: Harvard ;\n",
      "Adding to set of extracted relations\n",
      "==========\n",
      "\n",
      "=== Extracted Relation ===\n",
      "Input tokens: ['Chan', ',', '36', ',', 'graduated', 'from', 'Harvard', 'in', '2007', '.']\n",
      "Output Confidence: 0.9932754635810852 ; Subject: Chan ; Object: Harvard ;\n",
      "Adding to set of extracted relations\n",
      "==========\n",
      "Extracted annotations for  2  out of total  57  sentences\n",
      "Relations extracted from this website: 1 (Overall: 2)\n",
      "\n",
      "\n",
      "ERROR : Unable to parse https://www.thedailybeast.com/mark-zuckerberg-at-harvard-the-truth-behind-the-social-network\n",
      "\n",
      "\n",
      "URL ( 6 / 10): https://www.thecrimson.com/article/2004/6/10/mark-e-zuckerberg-06-the-whiz/\n",
      "Fetching text from url ...\n",
      "Webpage length (num characters): 12058\n",
      "Annotating the webpage using spacy...\n",
      "Extracted 125 sentences. Processing each sentence one by one to check for presence of right pair of named entity types; if so, will run the second pipeline ...\n",
      "Processed 0 / 125 sentences\n",
      "Processed 5 / 125 sentences\n",
      "Processed 10 / 125 sentences\n",
      "Processed 15 / 125 sentences\n",
      "Processed 20 / 125 sentences\n",
      "Processed 25 / 125 sentences\n",
      "Processed 30 / 125 sentences\n",
      "Processed 35 / 125 sentences\n",
      "Processed 40 / 125 sentences\n",
      "Processed 45 / 125 sentences\n",
      "Processed 50 / 125 sentences\n",
      "Processed 55 / 125 sentences\n",
      "Processed 60 / 125 sentences\n",
      "Processed 65 / 125 sentences\n",
      "Processed 70 / 125 sentences\n",
      "Processed 75 / 125 sentences\n",
      "Processed 80 / 125 sentences\n",
      "Processed 85 / 125 sentences\n",
      "Processed 90 / 125 sentences\n",
      "Processed 95 / 125 sentences\n",
      "Processed 100 / 125 sentences\n",
      "Processed 105 / 125 sentences\n",
      "Processed 110 / 125 sentences\n",
      "Processed 115 / 125 sentences\n",
      "Processed 120 / 125 sentences\n",
      "\n",
      "=== Extracted Relation ===\n",
      "Input tokens: ['Zuckerberg', 'attended', 'local', 'Ardsley', 'High', 'School', 'until', 'his', 'junior', 'year', ',', 'when', 'he', 'transferred', 'to', 'Phillips', 'Exeter', 'Academy', '.']\n",
      "Output Confidence: 0.9690465331077576 ; Subject: Zuckerberg ; Object: Phillips Exeter Academy ;\n",
      "Adding to set of extracted relations\n",
      "==========\n",
      "\n",
      "=== Extracted Relation ===\n",
      "Input tokens: ['Adam', 'D’Angelo', '(', 'now', 'a', 'student', 'at', 'CalTech', 'and', 'still', 'a', 'close', 'friend', ')']\n",
      "Output Confidence: 0.7787963151931763 ; Subject: Adam D’Angelo ; Object: CalTech ;\n",
      "Adding to set of extracted relations\n",
      "==========\n",
      "Extracted annotations for  2  out of total  125  sentences\n",
      "Relations extracted from this website: 2 (Overall: 3)\n",
      "\n",
      "\n",
      "ERROR : Unable to parse https://www.britannica.com/biography/Mark-Zuckerberg\n",
      "\n",
      "\n",
      "URL ( 8 / 10): https://www.npr.org/sections/thetwo-way/2017/05/26/530159142/mark-zuckerberg-tells-harvard-graduates-to-embrace-globalism-a-sense-of-purpose\n",
      "Fetching text from url ...\n",
      "Webpage length (num characters): 5463\n",
      "Annotating the webpage using spacy...\n",
      "Extracted 39 sentences. Processing each sentence one by one to check for presence of right pair of named entity types; if so, will run the second pipeline ...\n",
      "Processed 0 / 39 sentences\n",
      "Processed 5 / 39 sentences\n",
      "Processed 10 / 39 sentences\n",
      "Processed 15 / 39 sentences\n",
      "Processed 20 / 39 sentences\n",
      "Processed 25 / 39 sentences\n",
      "Processed 30 / 39 sentences\n",
      "Processed 35 / 39 sentences\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted annotations for  0  out of total  39  sentences\n",
      "Relations extracted from this website: 0 (Overall: 5)\n",
      "\n",
      "\n",
      "URL ( 9 / 10): https://www.bbc.com/news/world-us-canada-40053163\n",
      "Fetching text from url ...\n",
      "Webpage length (num characters): 5902\n",
      "Annotating the webpage using spacy...\n",
      "Extracted 42 sentences. Processing each sentence one by one to check for presence of right pair of named entity types; if so, will run the second pipeline ...\n",
      "Processed 0 / 42 sentences\n",
      "Processed 5 / 42 sentences\n",
      "Processed 10 / 42 sentences\n",
      "Processed 15 / 42 sentences\n",
      "Processed 20 / 42 sentences\n",
      "Processed 25 / 42 sentences\n",
      "Processed 30 / 42 sentences\n",
      "Processed 35 / 42 sentences\n",
      "Processed 40 / 42 sentences\n",
      "\n",
      "=== Extracted Relation ===\n",
      "Input tokens: ['Canada', 'Mark', 'Zuckerberg', 'gets', 'honorary', 'Harvard', 'degree', 'after', 'dropping', 'out', 'Published', '25', 'May', '2017']\n",
      "Output Confidence: 0.8870846629142761 ; Subject: Mark Zuckerberg ; Object: Harvard ;\n",
      "Adding to set of extracted relations\n",
      "==========\n",
      "Extracted annotations for  1  out of total  42  sentences\n",
      "Relations extracted from this website: 1 (Overall: 5)\n",
      "\n",
      "\n",
      "URL ( 10 / 10): https://nymag.com/intelligencer/2016/06/mark-zuckerberg-harvard-2006-reunion.html\n",
      "Fetching text from url ...\n",
      "Trimming webpage content from 24822 to 19992 characters\n",
      "Webpage length (num characters): 19992\n",
      "Annotating the webpage using spacy...\n",
      "Extracted 134 sentences. Processing each sentence one by one to check for presence of right pair of named entity types; if so, will run the second pipeline ...\n",
      "Processed 0 / 134 sentences\n",
      "Processed 5 / 134 sentences\n",
      "Processed 10 / 134 sentences\n",
      "Processed 15 / 134 sentences\n",
      "Processed 20 / 134 sentences\n",
      "Processed 25 / 134 sentences\n",
      "Processed 30 / 134 sentences\n",
      "Processed 35 / 134 sentences\n",
      "Processed 40 / 134 sentences\n",
      "Processed 45 / 134 sentences\n",
      "Processed 50 / 134 sentences\n",
      "Processed 55 / 134 sentences\n",
      "Processed 60 / 134 sentences\n",
      "Processed 65 / 134 sentences\n",
      "Processed 70 / 134 sentences\n",
      "Processed 75 / 134 sentences\n",
      "Processed 80 / 134 sentences\n",
      "Processed 85 / 134 sentences\n",
      "Processed 90 / 134 sentences\n",
      "Processed 95 / 134 sentences\n",
      "Processed 100 / 134 sentences\n",
      "Processed 105 / 134 sentences\n",
      "Processed 110 / 134 sentences\n",
      "Processed 115 / 134 sentences\n",
      "Processed 120 / 134 sentences\n",
      "Processed 125 / 134 sentences\n",
      "Processed 130 / 134 sentences\n",
      "Extracted annotations for  0  out of total  134  sentences\n",
      "Relations extracted from this website: 0 (Overall: 6)\n",
      "================== ALL RELATIONS for per:schools_attended ( 6 ) =================\n",
      "Confidence: 0.9932754635810852 \t\t| Subject: Chan \t\t| Object: Harvard\n",
      "Confidence: 0.9921025037765503 \t\t| Subject: David Razu Aznar \t\t| Object: the Kennedy School\n",
      "Confidence: 0.9893775582313538 \t\t| Subject: Zuckerberg \t\t| Object: Harvard\n",
      "Confidence: 0.9690465331077576 \t\t| Subject: Zuckerberg \t\t| Object: Phillips Exeter Academy\n",
      "Confidence: 0.8870846629142761 \t\t| Subject: Mark Zuckerberg \t\t| Object: Harvard\n",
      "Confidence: 0.7787963151931763 \t\t| Subject: Adam D’Angelo \t\t| Object: CalTech\n",
      "=========== Iteration: 1 - Query: Chan Harvard ===========\n",
      "\n",
      "\n",
      "URL ( 1 / 10): https://www.hsph.harvard.edu/\n",
      "Fetching text from url ...\n",
      "Webpage length (num characters): 5029\n",
      "Annotating the webpage using spacy...\n",
      "Extracted 18 sentences. Processing each sentence one by one to check for presence of right pair of named entity types; if so, will run the second pipeline ...\n",
      "Processed 0 / 18 sentences\n",
      "Processed 5 / 18 sentences\n",
      "Processed 10 / 18 sentences\n",
      "Processed 15 / 18 sentences\n",
      "Extracted annotations for  0  out of total  18  sentences\n",
      "Relations extracted from this website: 0 (Overall: 6)\n",
      "\n",
      "\n",
      "URL ( 2 / 10): https://en.wikipedia.org/wiki/Harvard_T.H._Chan_School_of_Public_Health\n",
      "Fetching text from url ...\n",
      "Trimming webpage content from 24961 to 19999 characters\n",
      "Webpage length (num characters): 19999\n",
      "Annotating the webpage using spacy...\n",
      "Extracted 223 sentences. Processing each sentence one by one to check for presence of right pair of named entity types; if so, will run the second pipeline ...\n",
      "Processed 0 / 223 sentences\n",
      "Processed 5 / 223 sentences\n",
      "Processed 10 / 223 sentences\n",
      "Processed 15 / 223 sentences\n",
      "Processed 20 / 223 sentences\n",
      "Processed 25 / 223 sentences\n",
      "Processed 30 / 223 sentences\n",
      "Processed 35 / 223 sentences\n",
      "Processed 40 / 223 sentences\n",
      "Processed 45 / 223 sentences\n",
      "Processed 50 / 223 sentences\n",
      "Processed 55 / 223 sentences\n",
      "Processed 60 / 223 sentences\n",
      "Processed 65 / 223 sentences\n",
      "Processed 70 / 223 sentences\n",
      "Processed 75 / 223 sentences\n",
      "Processed 80 / 223 sentences\n",
      "Processed 85 / 223 sentences\n",
      "Processed 90 / 223 sentences\n",
      "Processed 95 / 223 sentences\n",
      "Processed 100 / 223 sentences\n",
      "Processed 105 / 223 sentences\n",
      "Processed 110 / 223 sentences\n",
      "Processed 115 / 223 sentences\n",
      "Processed 120 / 223 sentences\n",
      "Processed 125 / 223 sentences\n",
      "Processed 130 / 223 sentences\n",
      "Processed 135 / 223 sentences\n",
      "Processed 140 / 223 sentences\n",
      "Processed 145 / 223 sentences\n",
      "Processed 150 / 223 sentences\n",
      "Processed 155 / 223 sentences\n",
      "Processed 160 / 223 sentences\n",
      "Processed 165 / 223 sentences\n",
      "Processed 170 / 223 sentences\n",
      "Processed 175 / 223 sentences\n",
      "Processed 180 / 223 sentences\n",
      "Processed 185 / 223 sentences\n",
      "Processed 190 / 223 sentences\n",
      "Processed 195 / 223 sentences\n",
      "Processed 200 / 223 sentences\n",
      "Processed 205 / 223 sentences\n",
      "Processed 210 / 223 sentences\n",
      "Processed 215 / 223 sentences\n",
      "Processed 220 / 223 sentences\n",
      "Extracted annotations for  0  out of total  223  sentences\n",
      "Relations extracted from this website: 0 (Overall: 6)\n",
      "\n",
      "\n",
      "URL ( 3 / 10): https://www.hsph.harvard.edu/admissions/degree-programs/\n",
      "Fetching text from url ...\n",
      "Webpage length (num characters): 18285\n",
      "Annotating the webpage using spacy...\n",
      "Extracted 96 sentences. Processing each sentence one by one to check for presence of right pair of named entity types; if so, will run the second pipeline ...\n",
      "Processed 0 / 96 sentences\n",
      "Processed 5 / 96 sentences\n",
      "Processed 10 / 96 sentences\n",
      "Processed 15 / 96 sentences\n",
      "Processed 20 / 96 sentences\n",
      "Processed 25 / 96 sentences\n",
      "Processed 30 / 96 sentences\n",
      "Processed 35 / 96 sentences\n",
      "Processed 40 / 96 sentences\n",
      "Processed 45 / 96 sentences\n",
      "Processed 50 / 96 sentences\n",
      "Processed 55 / 96 sentences\n",
      "Processed 60 / 96 sentences\n",
      "Processed 65 / 96 sentences\n",
      "Processed 70 / 96 sentences\n",
      "Processed 75 / 96 sentences\n",
      "Processed 80 / 96 sentences\n",
      "Processed 85 / 96 sentences\n",
      "Processed 90 / 96 sentences\n",
      "Processed 95 / 96 sentences\n",
      "\n",
      "=== Extracted Relation ===\n",
      "Input tokens: ['Admissions', 'Facebook', 'Twitter', 'Instagram', 'More', 'from', 'the', 'School', 'About', 'the', 'School', 'Academics', 'Research', 'Faculty', 'Student', 'Life', 'News', 'Alumni', 'Frontiers', 'Make', 'a', 'gift', 'Harvard', 'T.H.', 'Chan', 'School', 'of', 'Public', 'Health', '>', 'Admissions', '>', 'Degree', 'Programs', 'Degree', 'Programs']\n",
      "Output Confidence: 0.8503984808921814 ; Subject: Twitter Instagram ; Object: Harvard T.H. Chan School of Public Health > Admissions > Degree Programs Degree Programs ;\n",
      "Adding to set of extracted relations\n",
      "==========\n",
      "Extracted annotations for  1  out of total  96  sentences\n",
      "Relations extracted from this website: 1 (Overall: 6)\n",
      "\n",
      "\n",
      "ERROR : Unable to parse https://alumni.sph.harvard.edu/\n",
      "\n",
      "\n",
      "URL ( 5 / 10): https://www.hsph.harvard.edu/magazine/magazine_article/the-story-of-t-h-chan/\n",
      "Fetching text from url ...\n",
      "Webpage length (num characters): 6977\n",
      "Annotating the webpage using spacy...\n",
      "Extracted 54 sentences. Processing each sentence one by one to check for presence of right pair of named entity types; if so, will run the second pipeline ...\n",
      "Processed 0 / 54 sentences\n",
      "Processed 5 / 54 sentences\n",
      "Processed 10 / 54 sentences\n",
      "Processed 15 / 54 sentences\n",
      "Processed 20 / 54 sentences\n",
      "Processed 25 / 54 sentences\n",
      "Processed 30 / 54 sentences\n",
      "Processed 35 / 54 sentences\n",
      "Processed 40 / 54 sentences\n",
      "Processed 45 / 54 sentences\n",
      "Processed 50 / 54 sentences\n",
      "\n",
      "=== Extracted Relation ===\n",
      "Input tokens: ['Gerald', 'was', 'studying', 'at', 'Harvard', 'School', 'of', 'Public', 'Health', '.']\n",
      "Output Confidence: 0.9903839230537415 ; Subject: Gerald ; Object: Harvard School of Public Health ;\n",
      "Adding to set of extracted relations\n",
      "==========\n",
      "Extracted annotations for  1  out of total  54  sentences\n",
      "Relations extracted from this website: 1 (Overall: 7)\n",
      "\n",
      "\n",
      "URL ( 6 / 10): https://hcmph.sph.harvard.edu/hcmac/\n",
      "Fetching text from url ...\n",
      "Webpage length (num characters): 2497\n",
      "Annotating the webpage using spacy...\n",
      "Extracted 11 sentences. Processing each sentence one by one to check for presence of right pair of named entity types; if so, will run the second pipeline ...\n",
      "Processed 0 / 11 sentences\n",
      "Processed 5 / 11 sentences\n",
      "Processed 10 / 11 sentences\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted annotations for  0  out of total  11  sentences\n",
      "Relations extracted from this website: 0 (Overall: 8)\n",
      "\n",
      "\n",
      "URL ( 7 / 10): https://www.hsph.harvard.edu/faculty-research/\n",
      "Fetching text from url ...\n",
      "Webpage length (num characters): 2502\n",
      "Annotating the webpage using spacy...\n",
      "Extracted 9 sentences. Processing each sentence one by one to check for presence of right pair of named entity types; if so, will run the second pipeline ...\n",
      "Processed 0 / 9 sentences\n",
      "Processed 5 / 9 sentences\n",
      "Relations extracted from this website: 0 (Overall: 8)\n",
      "\n",
      "\n",
      "URL ( 8 / 10): https://news.harvard.edu/gazette/story/2021/07/harvard-chan-school-professor-discusses-delta-variant/\n",
      "Fetching text from url ...\n",
      "Webpage length (num characters): 7708\n",
      "Annotating the webpage using spacy...\n",
      "Extracted 52 sentences. Processing each sentence one by one to check for presence of right pair of named entity types; if so, will run the second pipeline ...\n",
      "Processed 0 / 52 sentences\n",
      "Processed 5 / 52 sentences\n",
      "Processed 10 / 52 sentences\n",
      "Processed 15 / 52 sentences\n",
      "Processed 20 / 52 sentences\n",
      "Processed 25 / 52 sentences\n",
      "Processed 30 / 52 sentences\n",
      "Processed 35 / 52 sentences\n",
      "Processed 40 / 52 sentences\n",
      "Processed 45 / 52 sentences\n",
      "Processed 50 / 52 sentences\n",
      "Extracted annotations for  0  out of total  52  sentences\n",
      "Relations extracted from this website: 0 (Overall: 8)\n",
      "\n",
      "\n",
      "URL ( 9 / 10): https://hcsra.sph.harvard.edu/\n",
      "Fetching text from url ...\n",
      "Webpage length (num characters): 2871\n",
      "Annotating the webpage using spacy...\n",
      "Extracted 10 sentences. Processing each sentence one by one to check for presence of right pair of named entity types; if so, will run the second pipeline ...\n",
      "Processed 0 / 10 sentences\n",
      "Processed 5 / 10 sentences\n",
      "Extracted annotations for  0  out of total  10  sentences\n",
      "Relations extracted from this website: 0 (Overall: 8)\n",
      "\n",
      "\n",
      "URL ( 10 / 10): https://engage.sph.harvard.edu/\n",
      "Fetching text from url ...\n",
      "Webpage length (num characters): 73\n",
      "Annotating the webpage using spacy...\n",
      "Extracted 1 sentences. Processing each sentence one by one to check for presence of right pair of named entity types; if so, will run the second pipeline ...\n",
      "Processed 0 / 1 sentences\n",
      "Relations extracted from this website: 0 (Overall: 8)\n",
      "================== ALL RELATIONS for per:schools_attended ( 8 ) =================\n",
      "Confidence: 0.9932754635810852 \t\t| Subject: Chan \t\t| Object: Harvard\n",
      "Confidence: 0.9921025037765503 \t\t| Subject: David Razu Aznar \t\t| Object: the Kennedy School\n",
      "Confidence: 0.9903839230537415 \t\t| Subject: Gerald \t\t| Object: Harvard School of Public Health\n",
      "Confidence: 0.9893775582313538 \t\t| Subject: Zuckerberg \t\t| Object: Harvard\n",
      "Confidence: 0.9690465331077576 \t\t| Subject: Zuckerberg \t\t| Object: Phillips Exeter Academy\n",
      "Confidence: 0.8870846629142761 \t\t| Subject: Mark Zuckerberg \t\t| Object: Harvard\n",
      "Confidence: 0.8503984808921814 \t\t| Subject: Twitter Instagram \t\t| Object: Harvard T.H. Chan School of Public Health > Admissions > Degree Programs Degree Programs\n",
      "Confidence: 0.7787963151931763 \t\t| Subject: Adam D’Angelo \t\t| Object: CalTech\n",
      "=========== Iteration: 1 - Query: David Razu Aznar the Kennedy School ===========\n",
      "\n",
      "\n",
      "URL ( 1 / 10): https://ash.harvard.edu/people/david-razu-aznar\n",
      "Fetching text from url ...\n",
      "Webpage length (num characters): 4669\n",
      "Annotating the webpage using spacy...\n",
      "Extracted 17 sentences. Processing each sentence one by one to check for presence of right pair of named entity types; if so, will run the second pipeline ...\n",
      "Processed 0 / 17 sentences\n",
      "Processed 5 / 17 sentences\n",
      "Processed 10 / 17 sentences\n",
      "Processed 15 / 17 sentences\n",
      "Extracted annotations for  0  out of total  17  sentences\n",
      "Relations extracted from this website: 0 (Overall: 8)\n",
      "\n",
      "\n",
      "URL ( 2 / 10): https://blogs.iadb.org/ciudades-sostenibles/en/author/drazuaznar/\n",
      "Fetching text from url ...\n",
      "Webpage length (num characters): 6470\n",
      "Annotating the webpage using spacy...\n",
      "Extracted 46 sentences. Processing each sentence one by one to check for presence of right pair of named entity types; if so, will run the second pipeline ...\n",
      "Processed 0 / 46 sentences\n",
      "Processed 5 / 46 sentences\n",
      "Processed 10 / 46 sentences\n",
      "Processed 15 / 46 sentences\n",
      "Processed 20 / 46 sentences\n",
      "Processed 25 / 46 sentences\n",
      "Processed 30 / 46 sentences\n",
      "Processed 35 / 46 sentences\n",
      "Processed 40 / 46 sentences\n",
      "Processed 45 / 46 sentences\n",
      "Extracted annotations for  0  out of total  46  sentences\n",
      "Relations extracted from this website: 0 (Overall: 8)\n",
      "\n",
      "\n",
      "URL ( 3 / 10): https://www.nbcboston.com/news/local/facebooks-zuckerberg-to-give-harvard-graduation-speech/19471/\n",
      "Fetching text from url ...\n",
      "Trimming webpage content from 26827 to 19992 characters\n",
      "Webpage length (num characters): 19992\n",
      "Annotating the webpage using spacy...\n",
      "Extracted 232 sentences. Processing each sentence one by one to check for presence of right pair of named entity types; if so, will run the second pipeline ...\n",
      "Processed 0 / 232 sentences\n",
      "Processed 5 / 232 sentences\n",
      "Processed 10 / 232 sentences\n",
      "Processed 15 / 232 sentences\n",
      "Processed 20 / 232 sentences\n",
      "Processed 25 / 232 sentences\n",
      "Processed 30 / 232 sentences\n",
      "Processed 35 / 232 sentences\n",
      "Processed 40 / 232 sentences\n",
      "Processed 45 / 232 sentences\n",
      "Processed 50 / 232 sentences\n",
      "Processed 55 / 232 sentences\n",
      "Processed 60 / 232 sentences\n",
      "Processed 65 / 232 sentences\n",
      "Processed 70 / 232 sentences\n",
      "Processed 75 / 232 sentences\n",
      "Processed 80 / 232 sentences\n",
      "Processed 85 / 232 sentences\n",
      "Processed 90 / 232 sentences\n",
      "Processed 95 / 232 sentences\n",
      "Processed 100 / 232 sentences\n",
      "Processed 105 / 232 sentences\n",
      "Processed 110 / 232 sentences\n",
      "Processed 115 / 232 sentences\n",
      "Processed 120 / 232 sentences\n",
      "Processed 125 / 232 sentences\n",
      "Processed 130 / 232 sentences\n",
      "Processed 135 / 232 sentences\n",
      "Processed 140 / 232 sentences\n",
      "Processed 145 / 232 sentences\n",
      "Processed 150 / 232 sentences\n",
      "Processed 155 / 232 sentences\n",
      "Processed 160 / 232 sentences\n",
      "Processed 165 / 232 sentences\n",
      "Processed 170 / 232 sentences\n",
      "Processed 175 / 232 sentences\n",
      "Processed 180 / 232 sentences\n",
      "Processed 185 / 232 sentences\n",
      "Processed 190 / 232 sentences\n",
      "Processed 195 / 232 sentences\n",
      "Processed 200 / 232 sentences\n",
      "Processed 205 / 232 sentences\n",
      "Processed 210 / 232 sentences\n",
      "Processed 215 / 232 sentences\n",
      "Processed 220 / 232 sentences\n",
      "Processed 225 / 232 sentences\n",
      "Processed 230 / 232 sentences\n",
      "Extracted annotations for  0  out of total  232  sentences\n",
      "Relations extracted from this website: 0 (Overall: 8)\n",
      "\n",
      "\n",
      "URL ( 4 / 10): https://www.iohbhutan.org/training\n",
      "Fetching text from url ...\n",
      "Webpage length (num characters): 3174\n",
      "Annotating the webpage using spacy...\n",
      "Extracted 22 sentences. Processing each sentence one by one to check for presence of right pair of named entity types; if so, will run the second pipeline ...\n",
      "Processed 0 / 22 sentences\n",
      "Processed 5 / 22 sentences\n",
      "Processed 10 / 22 sentences\n",
      "Processed 15 / 22 sentences\n",
      "Processed 20 / 22 sentences\n",
      "Extracted annotations for  0  out of total  22  sentences\n",
      "Relations extracted from this website: 0 (Overall: 8)\n",
      "\n",
      "\n",
      "URL ( 5 / 10): https://www.wikidata.org/wiki/Q18627433\n",
      "Fetching text from url ...\n",
      "Webpage length (num characters): 2321\n",
      "Annotating the webpage using spacy...\n",
      "Extracted 10 sentences. Processing each sentence one by one to check for presence of right pair of named entity types; if so, will run the second pipeline ...\n",
      "Processed 0 / 10 sentences\n",
      "Processed 5 / 10 sentences\n",
      "Relations extracted from this website: 0 (Overall: 8)\n",
      "\n",
      "\n",
      "URL ( 6 / 10): https://tr-ex.me/translation/chinese-english/david%E6%AF%95%E4%B8%9A\n",
      "Fetching text from url ...\n",
      "Webpage length (num characters): 7460\n",
      "Annotating the webpage using spacy...\n",
      "Extracted 76 sentences. Processing each sentence one by one to check for presence of right pair of named entity types; if so, will run the second pipeline ...\n",
      "Processed 0 / 76 sentences\n",
      "Processed 5 / 76 sentences\n",
      "Processed 10 / 76 sentences\n",
      "Processed 15 / 76 sentences\n",
      "Processed 20 / 76 sentences\n",
      "Processed 25 / 76 sentences\n",
      "Processed 30 / 76 sentences\n",
      "Processed 35 / 76 sentences\n",
      "Processed 40 / 76 sentences\n",
      "Processed 45 / 76 sentences\n",
      "Processed 50 / 76 sentences\n",
      "Processed 55 / 76 sentences\n",
      "Processed 60 / 76 sentences\n",
      "Processed 65 / 76 sentences\n",
      "Processed 70 / 76 sentences\n",
      "Processed 75 / 76 sentences\n",
      "\n",
      "=== Extracted Relation ===\n",
      "Input tokens: ['David', 'graduated', 'from', 'the', 'University', 'of', 'Pennsylvania', 'with', 'a', 'bachelor', \"'s\", 'degree', 'in', 'Digital', 'Media', 'Design', '.']\n",
      "Output Confidence: 0.9927801489830017 ; Subject: David ; Object: the University of Pennsylvania ;\n",
      "Adding to set of extracted relations\n",
      "==========\n",
      "\n",
      "=== Extracted Relation ===\n",
      "Input tokens: ['David', 'graduated', 'from', 'the', 'University', 'of', 'Pennsylvania', 'with', 'a', 'bachelor', \"'s\", 'degree', 'in', 'Digital', 'Media', 'Design', '.']\n",
      "Output Confidence: 0.9906908869743347 ; Subject: David ; Object: Digital Media Design ;\n",
      "Adding to set of extracted relations\n",
      "==========\n",
      "\n",
      "=== Extracted Relation ===\n",
      "Input tokens: ['David', 'MacKinnon', 'graduated', 'from', 'Queen', \"'s\", 'University', 'in', 'Canada', 'with', 'an', 'honours', 'degree', 'in', 'Life', 'Sciences', '.']\n",
      "Output Confidence: 0.99294114112854 ; Subject: David MacKinnon ; Object: Queen's University in Canada ;\n",
      "Adding to set of extracted relations\n",
      "==========\n",
      "\n",
      "=== Extracted Relation ===\n",
      "Input tokens: ['David', 'MacKinnon', 'graduated', 'from', 'Queen', \"'s\", 'University', 'in', 'Canada', 'with', 'an', 'honours', 'degree', 'in', 'Life', 'Sciences', '.']\n",
      "Output Confidence: 0.9847884774208069 ; Subject: David MacKinnon ; Object: Life Sciences ;\n",
      "Adding to set of extracted relations\n",
      "==========\n",
      "\n",
      "=== Extracted Relation ===\n",
      "Input tokens: ['David', 'graduated', 'from', 'Cambridge', 'University', 'in', '1997', 'with', 'the', 'Addison', '-']\n",
      "Output Confidence: 0.9933294653892517 ; Subject: David ; Object: Cambridge University ;\n",
      "Adding to set of extracted relations\n",
      "==========\n",
      "\n",
      "=== Extracted Relation ===\n",
      "Input tokens: ['I', 'met', 'David', 'Razu', 'Aznar', 'graduating', 'from', 'the', 'Kennedy', 'School', 'today', '.']\n",
      "Output Confidence: 0.9924318194389343 ; Subject: David Razu Aznar ; Object: the Kennedy School ;\n",
      "Adding to set of extracted relations\n",
      "==========\n",
      "\n",
      "=== Extracted Relation ===\n",
      "Input tokens: ['I', 'met', 'David', 'Razu', 'Aznar', 'graduating', 'from', 'the', 'Kennedy', 'School', 'today', '.']\n",
      "Output Confidence: 0.9924318194389343 ; Subject: David Razu Aznar ; Object: the Kennedy School ;\n",
      "Adding to set of extracted relations\n",
      "==========\n",
      "\n",
      "=== Extracted Relation ===\n",
      "Input tokens: ['I', 'met', 'David', 'Razu', 'Aznar', 'graduating', 'from', 'the', 'Kennedy', 'School', 'today', '.']\n",
      "Output Confidence: 0.9924318194389343 ; Subject: David Razu Aznar ; Object: the Kennedy School ;\n",
      "Adding to set of extracted relations\n",
      "==========\n",
      "\n",
      "=== Extracted Relation ===\n",
      "Input tokens: ['I', 'met', 'David', 'Razu', 'Aznar', 'graduating', 'from', 'the', 'Kennedy', 'School', 'today', '.']\n",
      "Output Confidence: 0.9924318194389343 ; Subject: David Razu Aznar ; Object: the Kennedy School ;\n",
      "Adding to set of extracted relations\n",
      "==========\n",
      "\n",
      "=== Extracted Relation ===\n",
      "Input tokens: ['Schwartz在休斯顿大学', '毕业', '前两年创办了自己的公司', 'David', 'SchwartzEnterprises', '。', 'Schwartz', 'started', 'his', 'own', 'company', 'David', 'Schwartz', 'Enterprises', '2', 'years', 'before', 'graduating', 'from', 'the', 'University', 'of', 'Houston', '.']\n",
      "Output Confidence: 0.8941189050674438 ; Subject: David SchwartzEnterprises ; Object: the University of Houston ;\n",
      "Adding to set of extracted relations\n",
      "==========\n",
      "\n",
      "=== Extracted Relation ===\n",
      "Input tokens: ['Schwartz', 'started', 'his', 'own', 'company', 'David', 'Schwartz', 'Enterprises', '2', 'years', 'before', 'graduating', 'from', 'the', 'University', 'of', 'Houston', '.']\n",
      "Output Confidence: 0.9928805232048035 ; Subject: Schwartz ; Object: the University of Houston ;\n",
      "Adding to set of extracted relations\n",
      "==========\n",
      "\n",
      "=== Extracted Relation ===\n",
      "Input tokens: ['毕业', '于现代语言的ucd，并在20世纪70年代初期以德语和文学方式毕业', '。', 'David', 'Donoghue', 'graduated', 'from', 'UCD', 'with', 'a', 'BA', 'in', 'Modern', 'Languages', 'and', 'a', 'MA', 'in', 'German', 'Language', 'and', 'Literature', 'in', 'the', 'early', '1970s', '.']\n",
      "Output Confidence: 0.9638055562973022 ; Subject: 于现代语言的ucd，并在20世纪70年代初期以德语和文学方式毕业 ; Object: UCD ;\n",
      "Adding to set of extracted relations\n",
      "==========\n",
      "\n",
      "=== Extracted Relation ===\n",
      "Input tokens: ['David', 'Donoghue', 'graduated', 'from', 'UCD', 'with', 'a', 'BA', 'in', 'Modern', 'Languages', 'and', 'a', 'MA', 'in', 'German', 'Language', 'and', 'Literature', 'in', 'the', 'early', '1970s', '.']\n",
      "Output Confidence: 0.9924204349517822 ; Subject: David Donoghue ; Object: UCD ;\n",
      "Adding to set of extracted relations\n",
      "==========\n",
      "\n",
      "=== Extracted Relation ===\n",
      "Input tokens: ['David', 'Donoghue', 'graduated', 'from', 'UCD', 'with', 'a', 'BA', 'in', 'Modern', 'Languages', 'and', 'a', 'MA', 'in', 'German', 'Language', 'and', 'Literature', 'in', 'the', 'early', '1970s', '.']\n",
      "Output Confidence: 0.9728309512138367 ; Subject: David Donoghue ; Object: German Language and Literature ;\n",
      "Adding to set of extracted relations\n",
      "==========\n",
      "\n",
      "=== Extracted Relation ===\n",
      "Input tokens: ['Other', 'distinguished', 'graduates', 'include', 'Michelle', 'Obama', 'actors', 'Jimmy', 'Stewart', 'and', 'David', 'Duchovny', 'Google', 'chairman', 'Eric', 'Schmidt', 'and', 'Apollo', 'astronaut', 'Pete', 'Conrad', '.']\n",
      "Output Confidence: 0.9411132335662842 ; Subject: Michelle Obama ; Object: Apollo ;\n",
      "Adding to set of extracted relations\n",
      "==========\n",
      "\n",
      "=== Extracted Relation ===\n",
      "Input tokens: ['Other', 'distinguished', 'graduates', 'include', 'Michelle', 'Obama', 'actors', 'Jimmy', 'Stewart', 'and', 'David', 'Duchovny', 'Google', 'chairman', 'Eric', 'Schmidt', 'and', 'Apollo', 'astronaut', 'Pete', 'Conrad', '.']\n",
      "Output Confidence: 0.8204360008239746 ; Subject: Eric Schmidt ; Object: Apollo ;\n",
      "Adding to set of extracted relations\n",
      "==========\n",
      "Extracted annotations for  10  out of total  76  sentences\n",
      "Relations extracted from this website: 13 (Overall: 8)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "URL ( 7 / 10): https://www.taiwannews.com.tw/en/news/3172844\n",
      "Fetching text from url ...\n",
      "Trimming webpage content from 23129 to 19995 characters\n",
      "Webpage length (num characters): 19995\n",
      "Annotating the webpage using spacy...\n",
      "Extracted 245 sentences. Processing each sentence one by one to check for presence of right pair of named entity types; if so, will run the second pipeline ...\n",
      "Processed 0 / 245 sentences\n",
      "Processed 5 / 245 sentences\n",
      "Processed 10 / 245 sentences\n",
      "Processed 15 / 245 sentences\n",
      "Processed 20 / 245 sentences\n",
      "Processed 25 / 245 sentences\n",
      "Processed 30 / 245 sentences\n",
      "Processed 35 / 245 sentences\n",
      "Processed 40 / 245 sentences\n",
      "Processed 45 / 245 sentences\n",
      "Processed 50 / 245 sentences\n",
      "Processed 55 / 245 sentences\n",
      "Processed 60 / 245 sentences\n",
      "Processed 65 / 245 sentences\n",
      "Processed 70 / 245 sentences\n",
      "Processed 75 / 245 sentences\n",
      "Processed 80 / 245 sentences\n",
      "Processed 85 / 245 sentences\n",
      "Processed 90 / 245 sentences\n",
      "Processed 95 / 245 sentences\n",
      "Processed 100 / 245 sentences\n",
      "Processed 105 / 245 sentences\n",
      "Processed 110 / 245 sentences\n",
      "Processed 115 / 245 sentences\n",
      "Processed 120 / 245 sentences\n",
      "Processed 125 / 245 sentences\n",
      "Processed 130 / 245 sentences\n",
      "Processed 135 / 245 sentences\n",
      "Processed 140 / 245 sentences\n",
      "Processed 145 / 245 sentences\n",
      "Processed 150 / 245 sentences\n",
      "Processed 155 / 245 sentences\n",
      "Processed 160 / 245 sentences\n",
      "Processed 165 / 245 sentences\n",
      "Processed 170 / 245 sentences\n",
      "Processed 175 / 245 sentences\n",
      "Processed 180 / 245 sentences\n",
      "Processed 185 / 245 sentences\n",
      "Processed 190 / 245 sentences\n",
      "Processed 195 / 245 sentences\n",
      "Processed 200 / 245 sentences\n",
      "Processed 205 / 245 sentences\n",
      "Processed 210 / 245 sentences\n",
      "Processed 215 / 245 sentences\n",
      "Processed 220 / 245 sentences\n",
      "Processed 225 / 245 sentences\n",
      "Processed 230 / 245 sentences\n",
      "Processed 235 / 245 sentences\n",
      "Processed 240 / 245 sentences\n",
      "\n",
      "=== Extracted Relation ===\n",
      "Input tokens: ['I', 'met', 'David', 'Razu', 'Aznar', ',', 'graduating', 'from', 'the', 'Kennedy', 'School', 'today', '.']\n",
      "Output Confidence: 0.9921025037765503 ; Subject: David Razu Aznar ; Object: the Kennedy School ;\n",
      "Adding to set of extracted relations\n",
      "==========\n",
      "Extracted annotations for  1  out of total  245  sentences\n",
      "Relations extracted from this website: 1 (Overall: 20)\n",
      "\n",
      "\n",
      "URL ( 8 / 10): https://www.hks.harvard.edu/more/student-life/test-student-stories/attending-hks-mid-career-student\n",
      "Fetching text from url ...\n",
      "Webpage length (num characters): 8499\n",
      "Annotating the webpage using spacy...\n",
      "Extracted 56 sentences. Processing each sentence one by one to check for presence of right pair of named entity types; if so, will run the second pipeline ...\n",
      "Processed 0 / 56 sentences\n",
      "Processed 5 / 56 sentences\n",
      "Processed 10 / 56 sentences\n",
      "Processed 15 / 56 sentences\n",
      "Processed 20 / 56 sentences\n",
      "Processed 25 / 56 sentences\n",
      "Processed 30 / 56 sentences\n",
      "Processed 35 / 56 sentences\n",
      "Processed 40 / 56 sentences\n",
      "Processed 45 / 56 sentences\n",
      "Processed 50 / 56 sentences\n",
      "Processed 55 / 56 sentences\n",
      "Extracted annotations for  0  out of total  56  sentences\n",
      "Relations extracted from this website: 0 (Overall: 20)\n",
      "\n",
      "\n",
      "URL ( 9 / 10): https://es.wikipedia.org/wiki/David_Raz%C3%BA_Aznar\n",
      "Fetching text from url ...\n",
      "Webpage length (num characters): 8298\n",
      "Annotating the webpage using spacy...\n",
      "Extracted 58 sentences. Processing each sentence one by one to check for presence of right pair of named entity types; if so, will run the second pipeline ...\n",
      "Processed 0 / 58 sentences\n",
      "Processed 5 / 58 sentences\n",
      "Processed 10 / 58 sentences\n",
      "Processed 15 / 58 sentences\n",
      "Processed 20 / 58 sentences\n",
      "Processed 25 / 58 sentences\n",
      "Processed 30 / 58 sentences\n",
      "Processed 35 / 58 sentences\n",
      "Processed 40 / 58 sentences\n",
      "Processed 45 / 58 sentences\n",
      "Processed 50 / 58 sentences\n",
      "Processed 55 / 58 sentences\n",
      "Extracted annotations for  0  out of total  58  sentences\n",
      "Relations extracted from this website: 0 (Overall: 20)\n",
      "\n",
      "\n",
      "URL ( 10 / 10): https://www.hks.harvard.edu/alumni/about/publications/hks2019report\n",
      "Fetching text from url ...\n",
      "Trimming webpage content from 100395 to 19999 characters\n",
      "Webpage length (num characters): 19999\n",
      "Annotating the webpage using spacy...\n",
      "Extracted 220 sentences. Processing each sentence one by one to check for presence of right pair of named entity types; if so, will run the second pipeline ...\n",
      "Processed 0 / 220 sentences\n",
      "Processed 5 / 220 sentences\n",
      "Processed 10 / 220 sentences\n",
      "Processed 15 / 220 sentences\n",
      "Processed 20 / 220 sentences\n",
      "Processed 25 / 220 sentences\n",
      "Processed 30 / 220 sentences\n",
      "Processed 35 / 220 sentences\n",
      "Processed 40 / 220 sentences\n",
      "Processed 45 / 220 sentences\n",
      "Processed 50 / 220 sentences\n",
      "Processed 55 / 220 sentences\n",
      "Processed 60 / 220 sentences\n",
      "Processed 65 / 220 sentences\n",
      "Processed 70 / 220 sentences\n",
      "Processed 75 / 220 sentences\n",
      "Processed 80 / 220 sentences\n",
      "Processed 85 / 220 sentences\n",
      "Processed 90 / 220 sentences\n",
      "Processed 95 / 220 sentences\n",
      "Processed 100 / 220 sentences\n",
      "Processed 105 / 220 sentences\n",
      "Processed 110 / 220 sentences\n",
      "Processed 115 / 220 sentences\n",
      "Processed 120 / 220 sentences\n",
      "Processed 125 / 220 sentences\n",
      "Processed 130 / 220 sentences\n",
      "Processed 135 / 220 sentences\n",
      "Processed 140 / 220 sentences\n",
      "Processed 145 / 220 sentences\n",
      "Processed 150 / 220 sentences\n",
      "Processed 155 / 220 sentences\n",
      "Processed 160 / 220 sentences\n",
      "Processed 165 / 220 sentences\n",
      "Processed 170 / 220 sentences\n",
      "Processed 175 / 220 sentences\n",
      "Processed 180 / 220 sentences\n",
      "Processed 185 / 220 sentences\n",
      "Processed 190 / 220 sentences\n",
      "Processed 195 / 220 sentences\n",
      "Processed 200 / 220 sentences\n",
      "Processed 205 / 220 sentences\n",
      "Processed 210 / 220 sentences\n",
      "Processed 215 / 220 sentences\n",
      "Extracted annotations for  0  out of total  220  sentences\n",
      "Relations extracted from this website: 0 (Overall: 20)\n",
      "================== ALL RELATIONS for per:schools_attended ( 20 ) =================\n",
      "Confidence: 0.9933294653892517 \t\t| Subject: David \t\t| Object: Cambridge University\n",
      "Confidence: 0.9932754635810852 \t\t| Subject: Chan \t\t| Object: Harvard\n",
      "Confidence: 0.99294114112854 \t\t| Subject: David MacKinnon \t\t| Object: Queen's University in Canada\n",
      "Confidence: 0.9928805232048035 \t\t| Subject: Schwartz \t\t| Object: the University of Houston\n",
      "Confidence: 0.9927801489830017 \t\t| Subject: David \t\t| Object: the University of Pennsylvania\n",
      "Confidence: 0.9924318194389343 \t\t| Subject: David Razu Aznar \t\t| Object: the Kennedy School\n",
      "Confidence: 0.9924204349517822 \t\t| Subject: David Donoghue \t\t| Object: UCD\n",
      "Confidence: 0.9906908869743347 \t\t| Subject: David \t\t| Object: Digital Media Design\n",
      "Confidence: 0.9903839230537415 \t\t| Subject: Gerald \t\t| Object: Harvard School of Public Health\n",
      "Confidence: 0.9893775582313538 \t\t| Subject: Zuckerberg \t\t| Object: Harvard\n",
      "Confidence: 0.9847884774208069 \t\t| Subject: David MacKinnon \t\t| Object: Life Sciences\n",
      "Confidence: 0.9728309512138367 \t\t| Subject: David Donoghue \t\t| Object: German Language and Literature\n",
      "Confidence: 0.9690465331077576 \t\t| Subject: Zuckerberg \t\t| Object: Phillips Exeter Academy\n",
      "Confidence: 0.9638055562973022 \t\t| Subject: 于现代语言的ucd，并在20世纪70年代初期以德语和文学方式毕业 \t\t| Object: UCD\n",
      "Confidence: 0.9411132335662842 \t\t| Subject: Michelle Obama \t\t| Object: Apollo\n",
      "Confidence: 0.8941189050674438 \t\t| Subject: David SchwartzEnterprises \t\t| Object: the University of Houston\n",
      "Confidence: 0.8870846629142761 \t\t| Subject: Mark Zuckerberg \t\t| Object: Harvard\n",
      "Confidence: 0.8503984808921814 \t\t| Subject: Twitter Instagram \t\t| Object: Harvard T.H. Chan School of Public Health > Admissions > Degree Programs Degree Programs\n",
      "Confidence: 0.8204360008239746 \t\t| Subject: Eric Schmidt \t\t| Object: Apollo\n",
      "Confidence: 0.7787963151931763 \t\t| Subject: Adam D’Angelo \t\t| Object: CalTech\n",
      "Total # of iterations = 1\n"
     ]
    }
   ],
   "source": [
    "while len(result) <= k:\n",
    "    current_query = search.get_query(query, result, previous_queries)\n",
    "    display.display_iteration(i, current_query)\n",
    "    proposed_sites = search.query(api_key, engine_id, current_query)\n",
    "    for idx_site, proposed_site in enumerate(proposed_sites):\n",
    "        if proposed_site not in seen_sites:\n",
    "            text_site = search.scrape(idx_site, proposed_site)\n",
    "            if text_site is None:\n",
    "                display.error_parsing(proposed_site)\n",
    "                continue\n",
    "            trim_site = search.trim(text_site)\n",
    "            proposed_entities = entities.process(trim_site, nlp, spanbert, relation, threshold)\n",
    "            entities.add_entities(proposed_entities, result)\n",
    "            seen_sites.add(proposed_site)\n",
    "            previous_queries.add(current_query)\n",
    "    display.display_result(result, relation)\n",
    "    if len(result) == 0:\n",
    "        raise ValueError()\n",
    "display.display_final_iteration(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e679d233",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adv_db2",
   "language": "python",
   "name": "adv_db2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
