{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b667f28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ad9e5e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "html = urllib.request.urlopen('http://www.cs.columbia.edu/~gravano/cs6111/Proj2/index.html').read()\n",
    "soup = BeautifulSoup(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "11b70de0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cs6111: Project 2 COMS E6111-Advanced.   Database Systems Spring 2022 Project 2 Due Date: Thursday March 31, 5 p.m. ET Teams You will carry out this project in teams of two ..   You can do the project with your same teammate as for Project 1 and.   you are also welcome to switch teammates if you wish. In this case,.   please be considerate and notify your Project 1 teammate.   immediately . If you want to form a new team but can\\'t find.   a teammate, please follow these steps: Post a message on the Ed Discussion board asking for a. \\tteammate— the best way . Send email to Akhil right away (and definitely before Friday, March 4, at 5.       p.m. ET ) asking Akhil to pair you up with another.       student without a teammate. Akhil will make a best effort to.       find you a teammate. You do not need to notify us of your team composition. Instead, you.   and your teammate will indicate your team composition when you.   submit your project on Gradescope (click on \"Add Group Member\" after.   one of you has submitted your project).  You will upload your final.   electronic submission on Gradescope exactly once per team, rather.   than once per student. Important notes: If you decide to drop the class, or are even remotely.     considering doing so, please be considerate and notify.     your teammate immediately . On a related note, do not wait until the day before the deadline.     to start working on the project, just to realize then that your.     teammate has dropped the class or formed a new team. It is your.     responsibility to start working on the project and spot any.     problems with your teammate early on. You can do this project by yourself if you so wish. Be aware,.     however, that you will have to do exactly the same project as.     two-student teams will. Please check the Class Policies webpage for important.       information about what kinds of collaboration are allowed for.       projects, and how to compute the number of available grace late.       days for a team. Overview This project is about information extraction on the web, or the. task of extracting \"structured\" information that is embedded in. natural language text on the web. As we discussed in class,. information extraction has many applications and, notably, is becoming. increasingly important for web search. In this project, you will implement a version of. the Iterative Set Expansion (ISE) algorithm that we. described in class: for a target information extraction task, an. \"extraction confidence threshold,\" a \"seed query\" for the task, and a. desired number of tuples k , you will follow ISE,. starting with the seed query (which should correspond to a plausible. tuple for the relation to extract), to return k tuples extracted for the specified relation from web pages with at. least the given extraction confidence, and following the procedure. that we outline below. The objective of this project is to provide you with a hands-on. experience on how to (i) retrieve and parse webpages; (ii) prepare and. annotate text on the webpages for subsequent analysis; and (iii). extract structured information from the webpages. You will develop and run your project on the Google Cloud. infrastructure, using your LionMail account and VM as you did. for Project 1 . IMPORTANT NOTE: When you restart your VM to work. on this project, you must request at least 15 GB of RAM for the VM, so. that your system will run without any memory issues. To do this, in. the \"VM instances\" page of your Google Cloud account, click on the. VM\\'s name, then click on \"EDIT\" at the top,. select n1-standard-4 (4 vCPU, 15 GB memory) as the. \"Machine type,\" and click on \"Save.\" Description For this project, you will write a program that implements the ISE. algorithm over the web. Your program will rely on: The Google Custom Search API to search the. \\tweb. You used this API for Project 1 . The Beautiful Soup toolkit to extract the actual.       plain text from a given webpage, and ignore HTML tags, links,.       images, and all other content that would interfere with the.       information extraction process. You might need to.       install pip3 first: sudo apt update sudo apt install python3-pip And then Beautiful Soup proper: pip3 install beautifulsoup4 The spaCy library to process and annotate text through linguistic analysis.       (e.g., split sentences from paragraphs, tokenize text, detect.       entities). You will need to use Python 3.6 and follow these.       steps on your Google Cloud VM: sudo apt-get update pip3 install -U pip setuptools wheel pip3 install -U spacy python3 -m spacy download en_core_web_lg The SpanBERT classifier to extract the. \\tfollowing four types of relations from text documents: Schools_Attended (internal. \\tname: per:schools_attended ) Work_For (internal. \\tname: per:employee_of ) Live_In (internal name: per:cities_of_residence ) Top_Member_Employees (internal. \\tname: org:top_members/employees ) We have implemented the scripts for downloading and running the.     pre-trained SpanBERT classifier for the purpose.     of this project: git clone https://github.com/gkaramanolakis/SpanBERT cd SpanBERT pip3 install -r requirements.txt bash download_finetuned.sh Overall, your program should receive as input: Your Google Custom Search Engine JSON API Key from Project 1 Your Google Engine ID from Project 1 An integer r between 1 and 4, indicating the.       relation to extract: 1 is for Schools_Attended ,.       2 is for Work_For , 3 is.       for Live_In , and 4 is.       for Top_Member_Employees A real number t between 0 and 1, indicating.       the \"extraction confidence threshold,\" which is the minimum.       extraction confidence that we request for the tuples in the.       output A \"seed query\" q , which is a list of words in.       double quotes corresponding to a plausible tuple for the.       relation to extract (e.g., \"bill gates microsoft\" for relation.       Work_For) An integer k greater than 0, indicating the.       number of tuples that we request in the output Then, your program should perform the following steps: Initialize X , the set of extracted tuples, as.       the empty set. Query your Google Custom Search Engine to obtain the URLs for.       the top-10 webpages for query q ; you can reuse.       your own code from Project 1 for this part if you so wish. For each URL from the previous step that you have not.       processed before (you should skip already-seen URLs, even if.       this involves processing fewer than 10 webpages in this.       iteration): Retrieve the corresponding webpage; if you cannot retrieve. \\t  the webpage (e.g., because of a timeout), just skip it and. \\t  move on, even if this involves processing fewer than 10. \\t  webpages in this iteration. Extract the actual plain text from the webpage using Beautiful Soup . If the resulting plain text is longer than 20,000. \\t  characters, truncate the text to its first 20,000 characters. \\t  (for efficiency) and discard the rest. Use the spaCy library to split the text.     into sentences and extract named entities (e.g., PERSON,.     ORGANIZATION). See below for details on how to perform this step. Use the sentences and named entity pairs as input.     to SpanBERT to predict the corresponding.     relations, and extract all instances of the relation specified by.     input parameter r . See below for details on how.     to perform this step. Identify the tuples that have an associated extraction. \\t  confidence of at least t and add them to. \\t  set X . Remove exact duplicates from.       set X : if X contains tuples.       that are identical to each other, keep only the copy that has.       the highest extraction confidence and remove.       from X the duplicate copies. (You do not need.       to remove approximate duplicates, for simplicity.) If X contains at least k tuples, return the top-k such tuples sorted in.       decreasing order by extraction confidence, together with the.       extraction confidence of each tuple, and stop. (Alternatively,.       you can return all of the tuples in X sorted in.       decreasing order of extraction confidence, not just the.       top- k such tuples; this is what the reference.       implementation does.) Otherwise, select from X a.       tuple y such that (1) y has.       not been used for querying yet and (2) y has an.       extraction confidence that is highest among the tuples.       in X that have not yet been used for.       querying. (You can break ties arbitrarily.) Create a.       query q from tuple y by just.       concatenating the attribute values together, and go to Step.       2. If no such y tuple exists, then stop. (ISE.       has \"stalled\" before retrieving k high-confidence tuples.) Performing the Annotation and Information Extraction Steps Steps 3.d and 3.e above require that you use. the spaCy library and the. pre-trained SpanBERT classifier to annotate the plain. text from each webpage and extract tuples for the target. relation r . Relation extraction is a complex task that generally operates over. text that has been annotated with appropriate tools. In particular,. the spaCy library that you will use in this project.   provides a variety of text pre-processing tools (e.g., sentence. splitting, tokenization, named entity recognition). For your project, you should use spaCy for.   splitting the text to sentences and for named entity recognition for.   each of the sentences. You can find instructions on how to.   apply spaCy for this.   task here and in our example script (see below). After having identified named entities for a sentence,.   you should use the pre-trained SpanBERT classifier for relation extraction. SpanBERT is a BERT-based relation classifier that.   considers as input (1) a sentence; (2) a subject entity from the.   sentence; and (3) an object entity from the.   sentence. SpanBERT then returns the predicted.   relation and the respective confidence value. You can find.   instructions on how to apply SpanBERT for this.   task here and in our example script (see below). We have put together two minimal Python scripts, namely, spacy_help_functions.py and example_relations.py , that.   perform the full relation extraction pipeline, to illustrate how.   the spaCy library is integrated.   with SpanBERT . To run these scripts, you need to.   place them under the same directory as the spanbert.py file.   (provided here ). As an example, consider the following sentence and the output from.   the full information extraction process: Sentence: \"Bill Gates stepped down as chairman of Microsoft in.     February 2014 and assumed a new post as technology adviser to.     support the newly appointed CEO Satya Nadella.\" Script output: spaCy extracted entities: [(\\'Bill Gates\\', \\'PERSON\\'), (\\'Microsoft\\', \\'ORGANIZATION\\'), (\\'February 2014\\', \\'DATE\\'), (\\'Satya Nadella\\', \\'PERSON\\')] Candidate entity pairs: 1.  Subject: (\\'Bill Gates\\', \\'PERSON\\') \\xa0 Object: (\\'Microsoft\\', \\'ORGANIZATION\\') 2. Subject: (\\'Microsoft\\', \\'ORGANIZATION\\') \\xa0 Object: (\\'Bill Gates\\', \\'PERSON\\') 3. Subject: (\\'Bill Gates\\', \\'PERSON\\') \\xa0 Object: (\\'Satya Nadella\\', \\'PERSON\\') 4. Subject: (\\'Satya Nadella\\', \\'PERSON\\') \\xa0 Object: (\\'Bill Gates\\', \\'PERSON\\') 5. Subject: (\\'Microsoft\\', \\'ORGANIZATION\\') \\xa0 Object: (\\'Satya Nadella\\', \\'PERSON\\') 6. Subject: (\\'Satya Nadella\\', \\'PERSON\\') \\xa0 Object: (\\'Microsoft\\', \\'ORGANIZATION\\') SpanBERT extracted relations: 1. Subject: Bill Gates   \\xa0  Object: Microsoft    \\xa0   Relation: per:employee_of    \\xa0   Confidence: 1.00 2. Subject: Microsoft    \\xa0  Object: Bill Gates    \\xa0  Relation: org:top_members/employees  \\xa0   Confidence: 0.99 3. Subject: Bill Gates    \\xa0 Object: Satya Nadella\\xa0   Relation: no_relation \\xa0  Confidence: 1.00 4. Subject: Satya Nadella \\xa0 Object: Bill Gates   \\xa0   Relation: no_relation  \\xa0 Confidence: 0.52 5. Subject: Microsoft     \\xa0 Object: Satya Nadella  \\xa0 Relation: no_relation  \\xa0 Confidence: 0.99 6. Subject: Satya Nadella \\xa0 Object: Microsoft    \\xa0   Relation: per:employee_of  \\xa0     Confidence: 0.98 Note that in the above example, SpanBERT runs 6.   times for the same sentence, each time with a different entity.   pair. SpanBERT extracts relations for 3 entity.   pairs and predicts the no_relation type for the rest of.   the pairs (i.e., no relations were extracted).  Each relation type.   predicted by SpanBERT is listed together with the.   associated extraction confidence score. Unfortunately, the SpanBERT classifier.     is computationally expensive , so for efficiency.     you need to minimize its use. Specifically, you.     should not run SpanBERT over.     entity pairs that do not contain named entities of the right type.     for the relation of interest r . The required.     named entity types for each relation type are as follows: Schools_Attended : Subject: PERSON, Object: ORGANIZATION Work_For : Subject: PERSON, Object: ORGANIZATION Live_In : Subject: PERSON, Object: one of LOCATION, CITY,.     STATE_OR_PROVINCE, or COUNTRY Top_Member_Employees : Subject: ORGANIZATION, Object: PERSON As an example, consider extraction for.   the Work_For relation (internal.   name: per:employee_of ). You should only keep entity.   pairs where the subject entity type is PERSON and the object entity.   type is ORGANIZATION. By applying this constraint in the example.   sentence above (\"Bill Gates stepped down ... appointed CEO Satya.   Nadella.\") SpanBERT would run only for the first.   entity pair (\\'Bill Gates\\', \\'Microsoft\\') and the sixth entity pair.   (\\'Satya Nadella\\', \\'Microsoft\\').  Note that the subject and object.   entities might appear in either order in a sentence and this is.   fine. So to annotate the text, you should implement two.   steps .  First, you should use spaCY to.   identify the sentences in the webpage text together with the named.   entities, if any, that appear in each sentence.  Then, you should.   construct entity pairs and run the.   expensive SpanBERT model,.   separately only over each entity pair that contains.   both required named entities for the relation of interest, as.   specified above. IMPORTANT: You must not run SpanBERT for any entity pairs that are missing.   one or two entities of the type required by the relation. If a.   sentence is missing one or two entities of the type required by the.   relation, you should skip it and move to the next sentence. While running the second step over a sentence, SpanBERT looks for some predefined set of relations. in a sentence. We are interested in just the four relations mentioned. above. (If you are curious about the other relations available, please. check. the complete list as well as an article with a detailed description .) What to Submit and When Your Project 2 submission will consist of the following three. components, which you should submit on Gradescope by Thursday March 31. at 5 p.m. ET: Your well-commented Python code , which should.     follow the format of our reference implementation (see below) and.     run on your Google Cloud VM, set up as.     detailed here (but.     with more memory, etc. for this project as indicated above). Your.     implementation should be called using the same exact name.     and arguments as the reference implementation . A README file including the following.   information: Your name and Columbia UNI, and your teammate\\'s name and. \\tColumbia UNI A list of all the files that you are submitting A clear description of how to run your program. Note that. \\tyour project must compile/run in a Google Cloud VM set up exactly. \\tfollowing our instructions (but with more memory,. \\tetc. for this project as indicated above). Provide all. \\tcommands necessary to install the required software and. \\tdependencies for your program. A clear description of the internal design of your project,. \\texplaining the general structure of your code (i.e., what its. \\tmain high-level components are and what they do), as well as. \\tacknowledging and describing all external libraries that you. \\tuse in your code A detailed description of how you carried out Step 3 in the. \\t\"Description\" section above Your Google Custom Search Engine JSON API Key and. \\t  Engine ID (so we can test your project) Any additional information that you consider significant A transcript of the run of your program on.     input parameters: 2 0.7 \"bill gates microsoft\" 10 (i.e., for r=2,.     t=0.7, q=[bill gates microsoft], and k=10). The format of your.     transcript should closely follow the format of the reference.     implementation, and should print the same information (i.e.,.     number of characters, sentences, relations, etc.) as the.     corresponding session of the reference implementation. To submit your project, please follow these steps: Create a directory named proj2. Copy the source code files into the proj2 directory, and.       include all the other files that are necessary for your program.       to run. Tar and gzip the proj2 directory, to generate a single file.       proj2.tar.gz. Submit on Gradescope exactly three files: Your proj2.tar.gz file with your code, Your uncompressed README file, and Your uncompressed transcript file. Reference Implementation for Project 2 We created a reference implementation for this project. The.   reference implementation is called as follows: python3 project2.py <google api key> <google engine.     id> <r> <t> <q> <k> where: <google api key> is your Google Custom.     Search Engine JSON API Key (see above) <google engine id> is your Google.     Custom Search Engine ID (see above) <r> is an integer between 1 and 4,.     indicating the relation to extract: 1 is.     for Schools_Attended , 2 is.     for Work_For , 3 is for Live_In ,.     and 4 is for Top_Member_Employees <t> is a real number between 0 and 1,.       indicating the \"extraction confidence threshold,\" which is the.       minimum extraction confidence that we request for the tuples in.       the output <q> is a \"seed query,\" which is a list.       of words in double quotes corresponding to a plausible tuple for.       the relation to extract (e.g., \"bill gates microsoft\" for.       relation Work_For) <k> is an integer greater than 0,.       indicating the number of tuples that we request in the.       output Unfortunately, the SpanBERT classifier requires.   substantial amounts of memory to run. Therefore, a VM that could.   support many concurrent runs of the reference implementation, to.   accommodate the number of students in the class, would exceed our.   available Google Cloud budget. So rather than giving you direct.   access to the reference implementation, we provide.   the transcripts of a.   variety of runs of the reference implementation . We will keep.   adding and updating these transcripts periodically, so you have.   reasonably up-to-date runs available. Please adhere to the format of the reference implementation for.   your submission and your transcript file. Also, you can use the.   transcripts of the reference implementation to give you an idea of.   how good your overall system should be. Ideally, the number of.   querying iterations that your system takes to extract the number of.   tuples requested should be at least as low as that of our reference.   implementation. Grading for Project 2 A part of your grade will be based on the correctness of your. overall system. Another part of your grade will be based on the number. of iterations that your system takes to extract the number of tuples. requested: ideally, the number of querying iterations that your system. takes to extract the number of tuples requested should be at least as. low as that of our reference implementation. We will not grade you on. the run-time efficiency of each individual iteration, as long as you. correctly implement the two annotator \"steps\" described above; in. particular, note that you must not run the second. (expensive) step for all sentences, but rather you should restrict. that second step to only those sentences that satisfy the criteria. described above.  We will also grade your submission based on the. quality of your code, the quality of the README file, and the quality. of your transcript.'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(soup.stripped_strings).replace('\\n', '. ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8b69609b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<head>\n",
       " <meta content=\"NOINDEX, NOFOLLOW\" name=\"ROBOTS\"/>\n",
       " <meta content=\"NOARCHIVE\" name=\"ROBOTS\"/>\n",
       " \n",
       " <!-- Required meta tags -->\n",
       " <meta charset=\"utf-8\"/>\n",
       " <meta content=\"width=device-width, initial-scale=1\" name=\"viewport\"/>\n",
       " <!-- Bootstrap CSS -->\n",
       " <link href=\"https://maxcdn.bootstrapcdn.com/bootstrap/3.2.0/css/bootstrap.min.css\" rel=\"stylesheet\"/>\n",
       " <link href=\"https://maxcdn.bootstrapcdn.com/bootstrap/3.2.0/css/bootstrap-theme.min.css\" rel=\"stylesheet\"/>\n",
       " <link href=\"http://www.cs.columbia.edu/~gravano/style.css\" rel=\"stylesheet\" type=\"text/css\">\n",
       " </link></head>, <title>cs6111: Project 2</title>]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[s.extract() for s in soup(['style', 'script', '[document]', 'head', 'title'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "61812db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# [s.extract() for s in soup(['style', 'script', '[document]', 'head', 'title'])]\n",
    "visible_text = soup.getText()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4e14f87a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n\\n\\n\\ncs6111: Project 2\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCOMS E6111-Advanced\\n  Database SystemsSpring 2022\\nProject 2\\n\\nDue Date: Thursday March 31, 5 p.m. ET\\nTeams\\nYou will carry out this project in teams of two.\\n  You can do the project with your same teammate as for Project 1 and\\n  you are also welcome to switch teammates if you wish. In this case,\\n  please be considerate and notify your Project 1 teammate\\n  immediately. If you want to form a new team but can\\'t find\\n  a teammate, please follow these steps:\\n\\nPost a message on the Ed Discussion board asking for a\\n\\tteammate—the best way.\\n    \\nSend email to Akhil\\n      right away (and definitely before Friday, March 4, at 5\\n      p.m. ET) asking Akhil to pair you up with another\\n      student without a teammate. Akhil will make a best effort to\\n      find you a teammate.\\n    \\n\\nYou do not need to notify us of your team composition. Instead, you\\n  and your teammate will indicate your team composition when you\\n  submit your project on Gradescope (click on \"Add Group Member\" after\\n  one of you has submitted your project).  You will upload your final\\n  electronic submission on Gradescope exactly once per team, rather\\n  than once per student.\\nImportant notes:\\n\\nIf you decide to drop the class, or are even remotely\\n    considering doing so, please be considerate and notify\\n    your teammate immediately.\\n  \\nOn a related note, do not wait until the day before the deadline\\n    to start working on the project, just to realize then that your\\n    teammate has dropped the class or formed a new team. It is your\\n    responsibility to start working on the project and spot any\\n    problems with your teammate early on.\\n  \\nYou can do this project by yourself if you so wish. Be aware,\\n    however, that you will have to do exactly the same project as\\n    two-student teams will.\\n  \\nPlease check the Class Policies webpage for important\\n      information about what kinds of collaboration are allowed for\\n      projects, and how to compute the number of available grace late\\n      days for a team.\\n  \\n\\nOverview\\nThis project is about information extraction on the web, or the\\ntask of extracting \"structured\" information that is embedded in\\nnatural language text on the web. As we discussed in class,\\ninformation extraction has many applications and, notably, is becoming\\nincreasingly important for web search.\\nIn this project, you will implement a version of\\nthe Iterative Set Expansion (ISE) algorithm that we\\ndescribed in class: for a target information extraction task, an\\n\"extraction confidence threshold,\" a \"seed query\" for the task, and a\\ndesired number of tuples k, you will follow ISE,\\nstarting with the seed query (which should correspond to a plausible\\ntuple for the relation to extract), to return k\\ntuples extracted for the specified relation from web pages with at\\nleast the given extraction confidence, and following the procedure\\nthat we outline below.\\nThe objective of this project is to provide you with a hands-on\\nexperience on how to (i) retrieve and parse webpages; (ii) prepare and\\nannotate text on the webpages for subsequent analysis; and (iii)\\nextract structured information from the webpages.\\nYou will develop and run your project on the Google Cloud\\ninfrastructure, using your LionMail account and VM as you did\\nfor Project 1.\\nIMPORTANT NOTE: When you restart your VM to work\\non this project, you must request at least 15 GB of RAM for the VM, so\\nthat your system will run without any memory issues. To do this, in\\nthe \"VM instances\" page of your Google Cloud account, click on the\\nVM\\'s name, then click on \"EDIT\" at the top,\\nselect n1-standard-4 (4 vCPU, 15 GB memory) as the\\n\"Machine type,\" and click on \"Save.\"\\n\\nDescription\\nFor this project, you will write a program that implements the ISE\\nalgorithm over the web. Your program will rely on:\\n\\nThe Google Custom Search API to search the\\n\\tweb. You used this API for Project 1.\\n    \\nThe Beautiful Soup toolkit to extract the actual\\n      plain text from a given webpage, and ignore HTML tags, links,\\n      images, and all other content that would interfere with the\\n      information extraction process. You might need to\\n      install pip3 first:\\n\\n\\tsudo apt update\\n\\tsudo apt install python3-pip\\n\\n      And then Beautiful Soup proper:\\n\\n        pip3 install beautifulsoup4\\n      \\n\\nThe spaCy\\n      library to process and annotate text through linguistic analysis\\n      (e.g., split sentences from paragraphs, tokenize text, detect\\n      entities). You will need to use Python 3.6 and follow these\\n      steps on your Google Cloud VM:\\n\\n        sudo apt-get update\\n        pip3 install -U pip setuptools wheel\\n        pip3 install -U spacy\\n        python3 -m spacy download en_core_web_lg\\n      \\nThe SpanBERT classifier to extract the\\n\\tfollowing four types of relations from text documents:\\n      \\nSchools_Attended (internal\\n\\tname: per:schools_attended)\\n\\t\\nWork_For (internal\\n\\tname: per:employee_of)\\n\\t\\nLive_In (internal name:\\n\\t  per:cities_of_residence)\\n\\t\\nTop_Member_Employees (internal\\n\\tname: org:top_members/employees)\\n\\t\\n\\n    We have implemented the scripts for downloading and running the\\n    pre-trained SpanBERT classifier for the purpose\\n    of this project:\\n\\n        git clone https://github.com/gkaramanolakis/SpanBERT\\n        cd SpanBERT\\n        pip3 install -r requirements.txt\\n        bash download_finetuned.sh\\n\\n\\n\\nOverall, your program should receive as input:\\n\\nYour Google Custom Search Engine JSON API Key\\n      from Project 1\\n\\nYour Google Engine ID from Project 1\\n\\nAn integer r between 1 and 4, indicating the\\n      relation to extract: 1 is for Schools_Attended,\\n      2 is for Work_For, 3 is\\n      for Live_In, and 4 is\\n      for Top_Member_Employees\\n\\nA real number t between 0 and 1, indicating\\n      the \"extraction confidence threshold,\" which is the minimum\\n      extraction confidence that we request for the tuples in the\\n      output\\n    \\nA \"seed query\" q, which is a list of words in\\n      double quotes corresponding to a plausible tuple for the\\n      relation to extract (e.g., \"bill gates microsoft\" for relation\\n      Work_For)\\n    \\nAn integer k greater than 0, indicating the\\n      number of tuples that we request in the output\\n    \\n\\n Then, your program should perform the following steps:\\n\\nInitialize X, the set of extracted tuples, as\\n      the empty set.\\n    \\nQuery your Google Custom Search Engine to obtain the URLs for\\n      the top-10 webpages for query q; you can reuse\\n      your own code from Project 1 for this part if you so wish.\\n    \\nFor each URL from the previous step that you have not\\n      processed before (you should skip already-seen URLs, even if\\n      this involves processing fewer than 10 webpages in this\\n      iteration):\\n      \\nRetrieve the corresponding webpage; if you cannot retrieve\\n\\t  the webpage (e.g., because of a timeout), just skip it and\\n\\t  move on, even if this involves processing fewer than 10\\n\\t  webpages in this iteration.\\n\\t\\nExtract the actual plain text from the webpage using\\n\\t  Beautiful Soup.\\n\\t\\nIf the resulting plain text is longer than 20,000\\n\\t  characters, truncate the text to its first 20,000 characters\\n\\t  (for efficiency) and discard the rest.\\n\\t\\nUse the spaCy library to split the text\\n    into sentences and extract named entities (e.g., PERSON,\\n    ORGANIZATION). See below for details on how to perform this step.\\n\\t\\nUse the sentences and named entity pairs as input\\n    to SpanBERT to predict the corresponding\\n    relations, and extract all instances of the relation specified by\\n    input parameter r. See below for details on how\\n    to perform this step.\\n  \\nIdentify the tuples that have an associated extraction\\n\\t  confidence of at least t and add them to\\n\\t  set X.\\n\\t\\n\\n\\nRemove exact duplicates from\\n      set X: if X contains tuples\\n      that are identical to each other, keep only the copy that has\\n      the highest extraction confidence and remove\\n      from X the duplicate copies. (You do not need\\n      to remove approximate duplicates, for simplicity.)\\n    \\nIf X contains at least k\\n      tuples, return the top-k such tuples sorted in\\n      decreasing order by extraction confidence, together with the\\n      extraction confidence of each tuple, and stop. (Alternatively,\\n      you can return all of the tuples in X sorted in\\n      decreasing order of extraction confidence, not just the\\n      top-k such tuples; this is what the reference\\n      implementation does.)\\n    \\nOtherwise, select from X a\\n      tuple y such that (1) y has\\n      not been used for querying yet and (2) y has an\\n      extraction confidence that is highest among the tuples\\n      in X that have not yet been used for\\n      querying. (You can break ties arbitrarily.) Create a\\n      query q from tuple y by just\\n      concatenating the attribute values together, and go to Step\\n      2. If no such y tuple exists, then stop. (ISE\\n      has \"stalled\" before retrieving k\\n      high-confidence tuples.)\\n    \\n\\nPerforming the Annotation and Information Extraction Steps\\nSteps 3.d and 3.e above require that you use\\nthe spaCy library and the\\npre-trained SpanBERT classifier to annotate the plain\\ntext from each webpage and extract tuples for the target\\nrelation r.\\nRelation extraction is a complex task that generally operates over\\ntext that has been annotated with appropriate tools. In particular,\\nthe spaCy library that you will use in this project\\n  provides a variety of text pre-processing tools (e.g., sentence\\nsplitting, tokenization, named entity recognition).\\nFor your project, you should use spaCy for\\n  splitting the text to sentences and for named entity recognition for\\n  each of the sentences. You can find instructions on how to\\n  apply spaCy for this\\n  task here and in our example script (see below).\\n   \\nAfter having identified named entities for a sentence,\\n  you should use the pre-trained SpanBERT\\n  classifier for relation extraction.\\n  SpanBERT is a BERT-based relation classifier that\\n  considers as input (1) a sentence; (2) a subject entity from the\\n  sentence; and (3) an object entity from the\\n  sentence. SpanBERT then returns the predicted\\n  relation and the respective confidence value. You can find\\n  instructions on how to apply SpanBERT for this\\n  task here and in our example script (see below).\\n  \\nWe have put together two minimal Python scripts, namely,\\n  spacy_help_functions.py and\\n  example_relations.py, that\\n  perform the full relation extraction pipeline, to illustrate how\\n  the spaCy library is integrated\\n  with SpanBERT. To run these scripts, you need to\\n  place them under the same directory as the spanbert.py file\\n  (provided here).\\n\\nAs an example, consider the following sentence and the output from\\n  the full information extraction process:\\n\\nSentence: \"Bill Gates stepped down as chairman of Microsoft in\\n    February 2014 and assumed a new post as technology adviser to\\n    support the newly appointed CEO Satya Nadella.\"\\nScript output:\\n\\n        spaCy extracted entities: [(\\'Bill Gates\\', \\'PERSON\\'), (\\'Microsoft\\', \\'ORGANIZATION\\'), (\\'February 2014\\', \\'DATE\\'), (\\'Satya Nadella\\', \\'PERSON\\')]\\n        Candidate entity pairs:\\n        1.  Subject: (\\'Bill Gates\\', \\'PERSON\\') \\xa0 Object: (\\'Microsoft\\', \\'ORGANIZATION\\')\\n        2. Subject: (\\'Microsoft\\', \\'ORGANIZATION\\') \\xa0 Object: (\\'Bill Gates\\', \\'PERSON\\')\\n        3. Subject: (\\'Bill Gates\\', \\'PERSON\\') \\xa0 Object: (\\'Satya Nadella\\', \\'PERSON\\')\\n        4. Subject: (\\'Satya Nadella\\', \\'PERSON\\') \\xa0 Object: (\\'Bill Gates\\', \\'PERSON\\')\\n        5. Subject: (\\'Microsoft\\', \\'ORGANIZATION\\') \\xa0 Object: (\\'Satya Nadella\\', \\'PERSON\\')\\n        6. Subject: (\\'Satya Nadella\\', \\'PERSON\\') \\xa0 Object: (\\'Microsoft\\', \\'ORGANIZATION\\')\\n\\n\\t      SpanBERT extracted relations:\\n        1. Subject: Bill Gates   \\xa0  Object: Microsoft    \\xa0   Relation: per:employee_of    \\xa0   Confidence: 1.00\\n        2. Subject: Microsoft    \\xa0  Object: Bill Gates    \\xa0  Relation: org:top_members/employees  \\xa0   Confidence: 0.99\\n        3. Subject: Bill Gates    \\xa0 Object: Satya Nadella\\xa0   Relation: no_relation \\xa0  Confidence: 1.00\\n        4. Subject: Satya Nadella \\xa0 Object: Bill Gates   \\xa0   Relation: no_relation  \\xa0 Confidence: 0.52\\n        5. Subject: Microsoft     \\xa0 Object: Satya Nadella  \\xa0 Relation: no_relation  \\xa0 Confidence: 0.99\\n        6. Subject: Satya Nadella \\xa0 Object: Microsoft    \\xa0   Relation: per:employee_of  \\xa0     Confidence: 0.98\\n\\n\\n\\n Note that in the above example, SpanBERT runs 6\\n  times for the same sentence, each time with a different entity\\n  pair. SpanBERT extracts relations for 3 entity\\n  pairs and predicts the no_relation type for the rest of\\n  the pairs (i.e., no relations were extracted).  Each relation type\\n  predicted by SpanBERT is listed together with the\\n  associated extraction confidence score.\\nUnfortunately, the SpanBERT classifier\\n    is computationally expensive, so for efficiency\\n    you need to minimize its use. Specifically, you\\n    should not run SpanBERT over\\n    entity pairs that do not contain named entities of the right type\\n    for the relation of interest r. The required\\n    named entity types for each relation type are as follows:\\n\\nSchools_Attended: Subject: PERSON, Object: ORGANIZATION\\n    \\nWork_For: Subject: PERSON, Object: ORGANIZATION\\n    \\nLive_In: Subject: PERSON, Object: one of LOCATION, CITY,\\n    STATE_OR_PROVINCE, or COUNTRY\\n    \\nTop_Member_Employees: Subject: ORGANIZATION, Object: PERSON\\n    \\n\\n As an example, consider extraction for\\n  the Work_For relation (internal\\n  name: per:employee_of). You should only keep entity\\n  pairs where the subject entity type is PERSON and the object entity\\n  type is ORGANIZATION. By applying this constraint in the example\\n  sentence above (\"Bill Gates stepped down ... appointed CEO Satya\\n  Nadella.\") SpanBERT would run only for the first\\n  entity pair (\\'Bill Gates\\', \\'Microsoft\\') and the sixth entity pair\\n  (\\'Satya Nadella\\', \\'Microsoft\\').  Note that the subject and object\\n  entities might appear in either order in a sentence and this is\\n  fine.\\n\\n\\nSo to annotate the text, you should implement two\\n  steps.  First, you should use spaCY to\\n  identify the sentences in the webpage text together with the named\\n  entities, if any, that appear in each sentence.  Then, you should\\n  construct entity pairs and run the\\n  expensive SpanBERT model,\\n  separately only over each entity pair that contains\\n  both required named entities for the relation of interest, as\\n  specified above. IMPORTANT: You must not\\n  run SpanBERT for any entity pairs that are missing\\n  one or two entities of the type required by the relation. If a\\n  sentence is missing one or two entities of the type required by the\\n  relation, you should skip it and move to the next sentence.\\n\\nWhile running the second step over a sentence,\\nSpanBERT looks for some predefined set of relations\\nin a sentence. We are interested in just the four relations mentioned\\nabove. (If you are curious about the other relations available, please\\ncheck\\nthe complete list as well as\\n  an article with a detailed description.) \\nWhat to Submit and When\\nYour Project 2 submission will consist of the following three\\ncomponents, which you should submit on Gradescope by Thursday March 31\\nat 5 p.m. ET:\\n\\nYour well-commented Python code, which should\\n    follow the format of our reference implementation (see below) and\\n    run on your Google Cloud VM, set up as\\n    detailed here (but\\n    with more memory, etc. for this project as indicated above). Your\\n    implementation should be called using the same exact name\\n    and arguments as the reference implementation.\\n  \\nA README file including the following\\n  information:\\n    \\nYour name and Columbia UNI, and your teammate\\'s name and\\n\\tColumbia UNI\\n      \\nA list of all the files that you are submitting\\n      \\nA clear description of how to run your program. Note that\\n\\tyour project must compile/run in a Google Cloud VM\\n\\tset up exactly\\n\\tfollowing our instructions (but with more memory,\\n\\tetc. for this project as indicated above). Provide all\\n\\tcommands necessary to install the required software and\\n\\tdependencies for your program.\\n      \\nA clear description of the internal design of your project,\\n\\texplaining the general structure of your code (i.e., what its\\n\\tmain high-level components are and what they do), as well as\\n\\tacknowledging and describing all external libraries that you\\n\\tuse in your code\\n      \\nA detailed description of how you carried out Step 3 in the\\n\\t\"Description\" section above\\n      \\nYour Google Custom Search Engine JSON API Key and\\n\\t  Engine ID (so we can test your project)\\n      \\nAny additional information that you consider significant\\n      \\n\\n A transcript of the run of your program on\\n    input parameters:\\n    2 0.7 \"bill gates microsoft\" 10 (i.e., for r=2,\\n    t=0.7, q=[bill gates microsoft], and k=10). The format of your\\n    transcript should closely follow the format of the reference\\n    implementation, and should print the same information (i.e.,\\n    number of characters, sentences, relations, etc.) as the\\n    corresponding session of the reference implementation.\\n\\nTo submit your project, please follow these steps:\\n\\nCreate a directory named proj2.\\n    \\nCopy the source code files into the proj2 directory, and\\n      include all the other files that are necessary for your program\\n      to run.\\n    \\nTar and gzip the proj2 directory, to generate a single file\\n      proj2.tar.gz.\\n    \\nSubmit on Gradescope exactly three files:\\n\\nYour proj2.tar.gz file with your code,\\nYour uncompressed README file, and\\nYour uncompressed transcript file.\\n\\n\\n\\nReference Implementation for Project 2\\nWe created a reference implementation for this project. The\\n  reference implementation is called as follows:\\npython3 project2.py <google api key> <google engine\\n    id> <r> <t> <q> <k>\\n    where:\\n\\n<google api key> is your Google Custom\\n    Search Engine JSON API Key (see above)\\n<google engine id> is your Google\\n    Custom Search Engine ID (see above)\\n<r> is an integer between 1 and 4,\\n    indicating the relation to extract: 1 is\\n    for Schools_Attended, 2 is\\n    for Work_For, 3 is for Live_In,\\n    and 4 is for Top_Member_Employees\\n<t> is a real number between 0 and 1,\\n      indicating the \"extraction confidence threshold,\" which is the\\n      minimum extraction confidence that we request for the tuples in\\n      the output\\n<q> is a \"seed query,\" which is a list\\n      of words in double quotes corresponding to a plausible tuple for\\n      the relation to extract (e.g., \"bill gates microsoft\" for\\n      relation Work_For)\\n<k> is an integer greater than 0,\\n      indicating the number of tuples that we request in the\\n      output\\n\\nUnfortunately, the SpanBERT classifier requires\\n  substantial amounts of memory to run. Therefore, a VM that could\\n  support many concurrent runs of the reference implementation, to\\n  accommodate the number of students in the class, would exceed our\\n  available Google Cloud budget. So rather than giving you direct\\n  access to the reference implementation, we provide\\n  the transcripts of a\\n  variety of runs of the reference implementation. We will keep\\n  adding and updating these transcripts periodically, so you have\\n  reasonably up-to-date runs available.\\nPlease adhere to the format of the reference implementation for\\n  your submission and your transcript file. Also, you can use the\\n  transcripts of the reference implementation to give you an idea of\\n  how good your overall system should be. Ideally, the number of\\n  querying iterations that your system takes to extract the number of\\n  tuples requested should be at least as low as that of our reference\\n  implementation.\\nGrading for Project 2\\nA part of your grade will be based on the correctness of your\\noverall system. Another part of your grade will be based on the number\\nof iterations that your system takes to extract the number of tuples\\nrequested: ideally, the number of querying iterations that your system\\ntakes to extract the number of tuples requested should be at least as\\nlow as that of our reference implementation. We will not grade you on\\nthe run-time efficiency of each individual iteration, as long as you\\ncorrectly implement the two annotator \"steps\" described above; in\\nparticular, note that you must not run the second\\n(expensive) step for all sentences, but rather you should restrict\\nthat second step to only those sentences that satisfy the criteria\\ndescribed above.  We will also grade your submission based on the\\nquality of your code, the quality of the README file, and the quality\\nof your transcript.\\n\\n\\n\\n\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "visible_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdfb3135",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adv_db2",
   "language": "python",
   "name": "adv_db2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
